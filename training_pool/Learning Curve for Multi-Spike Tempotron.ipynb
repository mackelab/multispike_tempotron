{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve for Multi-Spike Tempotron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run the function-containing cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from main_gen import gen_features\n",
    "\n",
    "def gen_background_data(n, fr, duration): \n",
    "    dt = 0.001 #bin (s)\n",
    "    gen_bg = np.random.random((n, np.rint(duration/dt).astype(int)))<fr*dt\n",
    "    gen_bg = gen_bg.astype(int)\n",
    "    return gen_bg\n",
    "\n",
    "def kernel_fn(length, tau_mem, tau_syn, time_ij):\n",
    "    time = np.arange(0., length, 1.) #ms\n",
    "    kernel = np.zeros(length)\n",
    "    eta = tau_mem/tau_syn\n",
    "    V_norm = eta**(eta/(eta-1))/(eta-1)\n",
    "    for count in range(length):\n",
    "        kernel[count] = V_norm*(np.exp(-(time[count]-time_ij)/tau_mem)-np.exp(-(time[count]-time_ij)/tau_syn))\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### From voltage calculation code by Alex ###\n",
    "\n",
    "def get_memory_len(kernel_array, ratio):\n",
    "    arr = (kernel_array - ratio*kernel_array.max())[::-1]\n",
    "    memory_len = len(kernel_array) - np.searchsorted(arr, 0)\n",
    "    return memory_len\n",
    "\n",
    "def presyn_input(data):\n",
    "    datalen = data.shape[1]\n",
    "    presyn_input = np.zeros(data.shape)\n",
    "    for neuron, ith_bin in zip(*np.where(data)):\n",
    "        mem_len = min(syn_memory_len, datalen - ith_bin)\n",
    "        presyn_input[neuron,ith_bin:ith_bin+mem_len] += syn_kernel[:mem_len]\n",
    "    return presyn_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preset Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the required parameters, then run the cell below to generate features (in dictionary form), postsynaptic potential (syn_kernel), and spike-triggered reset (ref_kernel). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 500\n",
    "n_fea = 10 #number of different features\n",
    "fr = 5 #average firing rate of each input neuron (Hz)\n",
    "T_fea = 0.05 #T_fea: feature duration (s)\n",
    "dt = 0.001 #bin: discrete time in second\n",
    "tau_mem = 20\n",
    "tau_syn = 5\n",
    "time_ij = 0\n",
    "init_kernel_len = 200\n",
    "\n",
    "np.random.seed(1000000000)\n",
    "features = gen_features(n_fea, n, fr, dt, T_fea)\n",
    "\n",
    "syn_kernel = kernel_fn(init_kernel_len, tau_mem, tau_syn, time_ij)\n",
    "syn_memory_len = get_memory_len(syn_kernel, ratio=0.001)\n",
    "syn_kernel = syn_kernel[:syn_memory_len]\n",
    "\n",
    "ref_kernel = np.exp(- np.arange(1000) / tau_mem)\n",
    "ref_memory_len = get_memory_len(ref_kernel, ratio=0.001)\n",
    "ref_kernel = ref_kernel[:ref_memory_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Unweighted Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to generate unweighted inputs for n probe trials.\n",
    "\n",
    "DO NOT run this cell if you already have the file containing generated probe trials (learning_curve_inputs.npy)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_probe_trial = 100\n",
    "learning_curve_inputs = []\n",
    "\n",
    "for i in range(n_probe_trial): \n",
    "    np.random.seed(i*100000)\n",
    "    bg_data = gen_background_data(n, fr, 1.95)\n",
    "    data_null = np.insert(bg_data, 975, np.zeros((50, n)), axis = 1)\n",
    "    syn_input_null = presyn_input(data_null)\n",
    "    learning_curve_inputs.append(syn_input_null)\n",
    "\n",
    "np.save(\"learning_curve_inputs\", learning_curve_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to generate unweighted inputs for features.\n",
    "\n",
    "DO NOT run this cell if you already have the 'feature_inputs.npy' file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_inputs = []\n",
    "\n",
    "for feature, spikes in features.items():\n",
    "    new_spikes = np.append(spikes, np.zeros((n, 150)), axis = 1)\n",
    "    fea_input = presyn_input(new_spikes)\n",
    "    feature_inputs.append(fea_input)\n",
    "\n",
    "np.save(\"feature_inputs\", feature_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Computing Neural Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function-containing cells below. Uncomment the 'np.savez()' lines in neural_response and noisy_neural_response functions if the input data is large to avoid losing of computed results when encountering problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions for computing neural responses WITHOUT noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_fea(n_inputs, fea_input, T_fea):\n",
    "    inputs = np.copy(n_inputs)\n",
    "    start = int((n_inputs.shape[2] - T_fea*1000)/2)\n",
    "    for n in inputs:\n",
    "        n[:, start:start+fea_input.shape[1]] += fea_input\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_response(omega_list, n_unweighted_inputs, fea_inputs, T_fea, theta):\n",
    "    R_fea_list = []\n",
    "    tot_input = np.zeros((1000, 100, 2000))\n",
    "    count = 0\n",
    "    for cycle in omega_list:\n",
    "        tot_input[count] = (cycle[np.newaxis, :, np.newaxis] * n_unweighted_inputs[:, :, :]).sum(axis = 1)\n",
    "        count += 1\n",
    "    null_spike_count = np.zeros(tot_input.shape[:-1])\n",
    "    for ith_bin in range(tot_input.shape[-1]):\n",
    "        null_spike_mask = tot_input[:,:,ith_bin] >= theta\n",
    "        null_spike_count += null_spike_mask\n",
    "        mem_len = min(ref_memory_len, tot_input.shape[-1] - ith_bin)\n",
    "        tot_input[:,:,ith_bin:ith_bin+mem_len] -= null_spike_mask[:,:,np.newaxis] * ref_kernel[:mem_len]\n",
    "    R_null_list = np.mean(null_spike_count, axis = 1)/(n_unweighted_inputs.shape[2]/1000)\n",
    "    #np.savez(\"R_null_list\", R_null_list, null_spike_count)\n",
    "    for fea_input in fea_inputs:\n",
    "        fea_unweighted_inputs = add_fea(n_unweighted_inputs, fea_input, T_fea)\n",
    "        fea_tot_input = np.zeros((1000, 100, 2000))\n",
    "        count = 0\n",
    "        for cycle in omega_list:\n",
    "            fea_tot_input[count] = (cycle[np.newaxis, :, np.newaxis] * fea_unweighted_inputs[:, :, :]).sum(axis = 1)\n",
    "            count += 1\n",
    "        spike_count = np.zeros(fea_tot_input.shape[:-1])\n",
    "        for ith_bin in range(fea_tot_input.shape[-1]):\n",
    "            spike_mask = fea_tot_input[:,:,ith_bin] >= theta\n",
    "            spike_count += spike_mask\n",
    "            mem_len = min(ref_memory_len, fea_tot_input.shape[-1] - ith_bin)\n",
    "            fea_tot_input[:,:,ith_bin:ith_bin+mem_len] -= spike_mask[:,:,np.newaxis] * ref_kernel[:mem_len]\n",
    "        fea_spike_count = spike_count - null_spike_count\n",
    "        R_fea_list.append(np.mean(fea_spike_count, axis = 1))\n",
    "        #np.savez(\"R_fea_list\", R_fea_list)\n",
    "    return R_fea_list, R_null_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions for computing neural responses WITH noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly delete the spikes\n",
    "# delete spikes --> noise = 0\n",
    "# add spikes --> noise = 1\n",
    "\n",
    "def add_noise(raw_feature, probability, noise):\n",
    "    noisy_fea = np.copy(raw_feature)\n",
    "    spike = 0 if noise else 1\n",
    "    data = list(zip(*np.where(noisy_fea)))\n",
    "    test_data = list(zip(*np.where(noisy_fea == spike)))\n",
    "    for i, j in test_data:\n",
    "        if np.random.random()*len(test_data) <= probability*len(data):\n",
    "            noisy_fea[i, j] = noise\n",
    "    return noisy_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spike time jittering --> jitter within the range of feature duration: range = 0, 50 (bin)\n",
    "\n",
    "def add_jitter(raw_feature, mean, st_dev):\n",
    "    raw_fea = np.copy(raw_feature)\n",
    "    noisy_fea = np.zeros(raw_fea.shape)\n",
    "    time = np.where(raw_fea)[1]\n",
    "    jitter = [int(round(i)) for i in list(np.random.normal(mean, st_dev, time.shape[0]))]\n",
    "    new_time = time + jitter\n",
    "    count = 0\n",
    "    for t in new_time:\n",
    "        if t < 0:\n",
    "            new_time[count] = 0\n",
    "        elif t >= raw_fea.shape[1]:\n",
    "            new_time[count] = raw_fea.shape[1] - 1\n",
    "        count += 1\n",
    "    noisy_fea[(np.where(raw_fea)[0], new_time)] = 1\n",
    "    return noisy_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noisy_fea(n_inputs, raw_fea, T_fea, probability, noise, jitter_mean, jitter_SD):\n",
    "    inputs = np.copy(n_inputs)\n",
    "    start = int((inputs.shape[2] - T_fea*1000)/2)\n",
    "    for n_input in inputs:\n",
    "        noisy_fea = add_noise(raw_fea, probability, noise)\n",
    "        if jitter_SD:\n",
    "            noisy_fea = add_jitter(noisy_fea, jitter_mean, jitter_SD)\n",
    "        new_fea = np.append(noisy_fea, np.zeros((n, 150)), axis = 1)\n",
    "        fea_input = presyn_input(new_fea)\n",
    "        n_input[:, start:start+fea_input.shape[1]] += fea_input\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisy_neural_response(omega_list, n_unweighted_inputs, raw_feas, T_fea, theta, probability, noise, jitter_mean, jitter_SD):\n",
    "    R_fea_list = []\n",
    "    tot_input = np.zeros((1000, 100, 2000))\n",
    "    count = 0\n",
    "    for cycle in omega_list:\n",
    "        tot_input[count] = (cycle[np.newaxis, :, np.newaxis] * n_unweighted_inputs[:, :, :]).sum(axis = 1)\n",
    "        count += 1\n",
    "    null_spike_count = np.zeros(tot_input.shape[:-1])\n",
    "    for ith_bin in range(tot_input.shape[-1]):\n",
    "        null_spike_mask = tot_input[:,:,ith_bin] >= theta\n",
    "        null_spike_count += null_spike_mask\n",
    "        mem_len = min(ref_memory_len, tot_input.shape[-1] - ith_bin)\n",
    "        tot_input[:,:,ith_bin:ith_bin+mem_len] -= null_spike_mask[:,:,np.newaxis] * ref_kernel[:mem_len]\n",
    "    R_null_list = np.mean(null_spike_count, axis = 1)/(n_unweighted_inputs.shape[2]/1000)\n",
    "    #np.savez(\"R_null_list\", R_null_list, null_spike_count)\n",
    "    for feature, raw_fea in raw_feas.items():\n",
    "        fea_unweighted_inputs = add_noisy_fea(n_unweighted_inputs, raw_fea, T_fea, probability, noise, jitter_mean, jitter_SD) \n",
    "        fea_tot_input = np.zeros((1000, 100, 2000))\n",
    "        count = 0\n",
    "        for cycle in omega_list:\n",
    "            fea_tot_input[count] = (cycle[np.newaxis, :, np.newaxis] * fea_unweighted_inputs[:, :, :]).sum(axis = 1)\n",
    "            count += 1\n",
    "        spike_count = np.zeros(fea_tot_input.shape[:-1])\n",
    "        for ith_bin in range(fea_tot_input.shape[-1]):\n",
    "            spike_mask = fea_tot_input[:,:,ith_bin] >= theta\n",
    "            spike_count += spike_mask\n",
    "            mem_len = min(ref_memory_len, fea_tot_input.shape[-1] - ith_bin)\n",
    "            fea_tot_input[:,:,ith_bin:ith_bin+mem_len] -= spike_mask[:,:,np.newaxis] * ref_kernel[:mem_len]\n",
    "        fea_spike_count = spike_count - null_spike_count\n",
    "        R_fea_list.append(np.mean(fea_spike_count, axis = 1))\n",
    "        #np.savez(\"R_fea_list\", R_fea_list)\n",
    "    return R_fea_list, R_null_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below to compute the neural responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to compute the neural responses, which will be used to plot learning curves.\n",
    "\n",
    "Use 'theta_omegaXFXS_list.npy' for computing neural responses of updated synaptic efficacies obtained from multi-spike tempotron training (computed from gradient of theta critical). To compute the neural responses of synaptic efficacies obtained from correlation-based training, use 'corr_omegaXFXS_list.npy' by commenting the first line and uncommenting the second line of the cells.\n",
    "\n",
    "Also, remember to comment and uncommment the corresponding 'np.savez()' lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Neural Responses WITHOUT Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Identification of one feature with one spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "omega = np.load(\"theta_omega1F1S_list.npy\") # updated omega list after training\n",
    "#omega = np.load(\"corr_omega1F1S_list.npy\")\n",
    "l_curve_inputs = np.load(\"learning_curve_inputs.npy\") # unweighted inputs for a list of probe trials (n = 100)\n",
    "fea_inputs = np.load(\"feature_inputs.npy\") # unweighted inputs of features\n",
    "theta = 1 # threshold\n",
    "\n",
    "R_fea_list, R_null_list = neural_response(omega, l_curve_inputs, fea_inputs, T_fea, theta)\n",
    "np.savez(\"theta_neural_responses_1F1S\", R_fea_list, R_null_list)\n",
    "#np.savez(\"corr_neural_responses_1F1S\", R_fea_list, R_null_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Identification of one feature with multiple spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "omega = np.load(\"theta_omega1F5S_list.npy\") # updated omega list after training\n",
    "#omega = np.load(\"corr_omega1F5S_list.npy\")\n",
    "l_curve_inputs = np.load(\"learning_curve_inputs.npy\") # unweighted inputs for a list of probe trials (n = 100)\n",
    "fea_inputs = np.load(\"feature_inputs.npy\") # unweighted inputs of features\n",
    "theta = 1 # threshold\n",
    "\n",
    "R_fea_list, R_null_list = neural_response(omega, l_curve_inputs, fea_inputs, T_fea, theta)\n",
    "np.savez(\"theta_neural_responses_1F5S\", R_fea_list, R_null_list)\n",
    "#np.savez(\"corr_neural_responses_1F5S\", R_fea_list, R_null_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 Identification of multiple features with one spike per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "omega = np.load(\"theta_omega5F1S_list.npy\") # updated omega list after training\n",
    "#omega = np.load(\"corr_omega5F1S_list.npy\")\n",
    "l_curve_inputs = np.load(\"learning_curve_inputs.npy\") # unweighted inputs for a list of probe trials (n = 100)\n",
    "fea_inputs = np.load(\"feature_inputs.npy\") # unweighted inputs of features\n",
    "theta = 1 # threshold\n",
    "\n",
    "R_fea_list, R_null_list = neural_response(omega, l_curve_inputs, fea_inputs, T_fea, theta)\n",
    "np.savez(\"theta_neural_responses_5F1S\", R_fea_list, R_null_list)\n",
    "#np.savez(\"corr_neural_responses_5F1S\", R_fea_list, R_null_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 Identification of multiple features with different number of spikes for different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "omega = np.load(\"theta_omega5FmS_list.npy\") # updated omega list after training\n",
    "#omega = np.load(\"corr_omega5FmS_list.npy\")\n",
    "l_curve_inputs = np.load(\"learning_curve_inputs.npy\") # unweighted inputs for a list of probe trials (n = 100)\n",
    "fea_inputs = np.load(\"feature_inputs.npy\") # unweighted inputs of features\n",
    "theta = 1 # threshold\n",
    "\n",
    "R_fea_list, R_null_list = neural_response(omega, l_curve_inputs, fea_inputs, T_fea, theta)\n",
    "np.savez(\"theta_neural_responses_5FmS\", R_fea_list, R_null_list)\n",
    "#np.savez(\"corr_neural_responses_5FmS\", R_fea_list, R_null_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5 Identification of multiple features with a fixed number of spikes (>1) for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "omega = np.load(\"theta_omega5F5S_list.npy\") # updated omega list after training\n",
    "#omega = np.load(\"corr_omega5F5S_list.npy\")\n",
    "l_curve_inputs = np.load(\"learning_curve_inputs.npy\") # unweighted inputs for a list of probe trials (n = 100)\n",
    "fea_inputs = np.load(\"feature_inputs.npy\") # unweighted inputs of features\n",
    "theta = 1 # threshold\n",
    "\n",
    "R_fea_list, R_null_list = neural_response(omega, l_curve_inputs, fea_inputs, T_fea, theta)\n",
    "np.savez(\"theta_neural_responses_5F5S\", R_fea_list, R_null_list)\n",
    "#np.savez(\"corr_neural_responses_5F5S\", R_fea_list, R_null_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Neural Responses WITH Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Identification of one feature with one spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#omega = np.load(\"theta_omega1F1S_list.npy\") # updated omega list after training\n",
    "omega = np.load(\"corr_omega1F1S_list.npy\")\n",
    "l_curve_inputs = np.load(\"learning_curve_inputs.npy\") # unweighted inputs for a list of probe trials (n = 10)\n",
    "raw_fea = features # raw data of features (in dictionary form)\n",
    "theta = 1 # threshold\n",
    "probability = 0.2 # range = (0, 1)\n",
    "noise = 0 # delete spikes --> 0, add spikes --> 1\n",
    "jitter_mean = 0 # Gaussian distribution\n",
    "jitter_SD = 2 # Gaussian distribution (in number of bins)\n",
    "\n",
    "R_fea_list, R_null_list = noisy_neural_response(omega, l_curve_inputs, raw_fea, T_fea, theta, probability, noise, jitter_mean, jitter_SD) \n",
    "#np.savez(\"noisy_theta_neural_responses_1F1S\", R_fea_list, R_null_list)\n",
    "np.savez(\"noisy_corr_neural_responses_1F1S\", R_fea_list, R_null_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Identification of one feature with multiple spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#omega = np.load(\"theta_omega1F5S_list.npy\") # updated omega list after training\n",
    "omega = np.load(\"corr_omega1F5S_list.npy\")\n",
    "l_curve_inputs = np.load(\"learning_curve_inputs.npy\") # unweighted inputs for a list of probe trials (n = 10)\n",
    "raw_fea = features # raw data of features (in dictionary form)\n",
    "theta = 1 # threshold\n",
    "probability = 0.2 # range = (0, 1)\n",
    "noise = 0 # delete spikes --> 0, add spikes --> 1\n",
    "jitter_mean = 0 # Gaussian distribution\n",
    "jitter_SD = 2 # Gaussian distribution (in number of bins)\n",
    "\n",
    "R_fea_list, R_null_list = noisy_neural_response(omega, l_curve_inputs, raw_fea, T_fea, theta, probability, noise, jitter_mean, jitter_SD) \n",
    "#np.savez(\"noisy_theta_neural_responses_1F5S\", R_fea_list, R_null_list)\n",
    "np.savez(\"noisy_corr_neural_responses_1F5S\", R_fea_list, R_null_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Identification of multiple features with one spike per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#omega = np.load(\"theta_omega5F1S_list.npy\") # updated omega list after training\n",
    "omega = np.load(\"corr_omega5F1S_list.npy\")\n",
    "l_curve_inputs = np.load(\"learning_curve_inputs.npy\") # unweighted inputs for a list of probe trials (n = 10)\n",
    "raw_fea = features # raw data of features (in dictionary form)\n",
    "theta = 1 # threshold\n",
    "probability = 0.2 # range = (0, 1)\n",
    "noise = 0 # delete spikes --> 0, add spikes --> 1\n",
    "jitter_mean = 0 # Gaussian distribution\n",
    "jitter_SD = 2 # Gaussian distribution (in number of bins)\n",
    "\n",
    "R_fea_list, R_null_list = noisy_neural_response(omega, l_curve_inputs, raw_fea, T_fea, theta, probability, noise, jitter_mean, jitter_SD) \n",
    "#np.savez(\"noisy_theta_neural_responses_5F1S\", R_fea_list, R_null_list)\n",
    "np.savez(\"noisy_corr_neural_responses_5F1S\", R_fea_list, R_null_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 Identification of multiple features with different number of spikes for different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#omega = np.load(\"theta_omega5FmS_list.npy\") # updated omega list after training\n",
    "omega = np.load(\"corr_omega5FmS_list.npy\")\n",
    "l_curve_inputs = np.load(\"learning_curve_inputs.npy\") # unweighted inputs for a list of probe trials (n = 10)\n",
    "raw_fea = features # raw data of features (in dictionary form)\n",
    "theta = 1 # threshold\n",
    "probability = 0.2 # range = (0, 1)\n",
    "noise = 0 # delete spikes --> 0, add spikes --> 1\n",
    "jitter_mean = 0 # Gaussian distribution\n",
    "jitter_SD = 2 # Gaussian distribution (in number of bins)\n",
    "\n",
    "R_fea_list, R_null_list = noisy_neural_response(omega, l_curve_inputs, raw_fea, T_fea, theta, probability, noise, jitter_mean, jitter_SD) \n",
    "#np.savez(\"noisy_theta_neural_responses_5FmS\", R_fea_list, R_null_list)\n",
    "np.savez(\"noisy_corr_neural_responses_5FmS\", R_fea_list, R_null_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.5 Identification of multiple features with a fixed number of spikes (>1) for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#omega = np.load(\"theta_omega5F5S_list.npy\") # updated omega list after training\n",
    "omega = np.load(\"corr_omega5F5S_list.npy\")\n",
    "l_curve_inputs = np.load(\"learning_curve_inputs.npy\") # unweighted inputs for a list of probe trials (n = 10)\n",
    "raw_fea = features # raw data of features (in dictionary form)\n",
    "theta = 1 # threshold\n",
    "probability = 0.2 # range = (0, 1)\n",
    "noise = 0 # delete spikes --> 0, add spikes --> 1\n",
    "jitter_mean = 0 # Gaussian distribution\n",
    "jitter_SD = 2 # Gaussian distribution (in number of bins)\n",
    "\n",
    "R_fea_list, R_null_list = noisy_neural_response(omega, l_curve_inputs, raw_fea, T_fea, theta, probability, noise, jitter_mean, jitter_SD) \n",
    "#np.savez(\"noisy_theta_neural_responses_5F5S\", R_fea_list, R_null_list)\n",
    "np.savez(\"noisy_corr_neural_responses_5F5S\", R_fea_list, R_null_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
