{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve for Multi-Spike Tempotron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run the function-containing cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from main_gen import gen_features\n",
    "\n",
    "def gen_background_data(n, fr, duration): \n",
    "    dt = 0.001 #bin (s)\n",
    "    gen_bg = np.random.random((n, np.rint(duration/dt).astype(int)))<fr*dt\n",
    "    gen_bg = gen_bg.astype(int)\n",
    "    return gen_bg\n",
    "\n",
    "def kernel_fn(length, tau_mem, tau_syn, time_ij):\n",
    "    time = np.arange(0., length, 1.) #ms\n",
    "    kernel = np.zeros(length)\n",
    "    eta = tau_mem/tau_syn\n",
    "    V_norm = eta**(eta/(eta-1))/(eta-1)\n",
    "    for count in range(length):\n",
    "        kernel[count] = V_norm*(np.exp(-(time[count]-time_ij)/tau_mem)-np.exp(-(time[count]-time_ij)/tau_syn))\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### From voltage calculation code by Alex ###\n",
    "\n",
    "def get_memory_len(kernel_array, ratio):\n",
    "    arr = (kernel_array - ratio*kernel_array.max())[::-1]\n",
    "    memory_len = len(kernel_array) - np.searchsorted(arr, 0)\n",
    "    return memory_len\n",
    "\n",
    "def presyn_input(data):\n",
    "    datalen = data.shape[1]\n",
    "    presyn_input = np.zeros(data.shape)\n",
    "    for neuron, ith_bin in zip(*np.where(data)):\n",
    "        mem_len = min(syn_memory_len, datalen - ith_bin)\n",
    "        presyn_input[neuron,ith_bin:ith_bin+mem_len] += syn_kernel[:mem_len]\n",
    "    return presyn_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preset Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the required parameters, then run the cell below to generate features (in dictionary form), postsynaptic potential (syn_kernel), and spike-triggered reset (ref_kernel). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 500\n",
    "n_fea = 10 #number of different features\n",
    "fr = 5 #average firing rate of each input neuron (Hz)\n",
    "T_fea = 0.05 #T_fea: feature duration (s)\n",
    "dt = 0.001 #bin: discrete time in second\n",
    "tau_mem = 20\n",
    "tau_syn = 5\n",
    "time_ij = 0\n",
    "init_kernel_len = 200\n",
    "\n",
    "np.random.seed(1000000000)\n",
    "features = gen_features(n_fea, n, fr, dt, T_fea)\n",
    "\n",
    "syn_kernel = kernel_fn(init_kernel_len, tau_mem, tau_syn, time_ij)\n",
    "syn_memory_len = get_memory_len(syn_kernel, ratio=0.001)\n",
    "syn_kernel = syn_kernel[:syn_memory_len]\n",
    "\n",
    "ref_kernel = np.exp(- np.arange(1000) / tau_mem)\n",
    "ref_memory_len = get_memory_len(ref_kernel, ratio=0.001)\n",
    "ref_kernel = ref_kernel[:ref_memory_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Unweighted Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to generate unweighted inputs for n probe trials.\n",
    "\n",
    "DO NOT run this cell if you already have the file containing generated probe trials (learning_curve_inputs.npy)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_probe_trial = 100\n",
    "learning_curve_inputs = []\n",
    "\n",
    "for i in range(n_probe_trial): \n",
    "    np.random.seed(i*100000)\n",
    "    bg_data = gen_background_data(n, fr, 1.95)\n",
    "    data_null = np.insert(bg_data, 975, np.zeros((50, n)), axis = 1)\n",
    "    syn_input_null = presyn_input(data_null)\n",
    "    learning_curve_inputs.append(syn_input_null)\n",
    "\n",
    "np.save(\"learning_curve_inputs\", learning_curve_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to generate unweighted inputs for features.\n",
    "\n",
    "DO NOT run this cell if you already have the 'feature_inputs.npy' file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_inputs = []\n",
    "\n",
    "for feature, spikes in features.items():\n",
    "    new_spikes = np.append(spikes, np.zeros((n, 150)), axis = 1)\n",
    "    fea_input = presyn_input(new_spikes)\n",
    "    feature_inputs.append(fea_input)\n",
    "\n",
    "np.save(\"feature_inputs\", feature_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Computing Neural Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function-containing cells below. Uncomment the 'np.savez()' lines in neural_response function if the input data is large to avoid losing of computed results when encountering problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_fea(n_inputs, fea_input, T_fea):\n",
    "    start = (n_inputs.shape[2] - T_fea*1000)/2\n",
    "    for n in n_inputs:\n",
    "        n[:, start:start+fea_input.shape[1]] += fea_input\n",
    "    return n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_response(omega_list, n_unweighted_inputs, fea_inputs, T_fea, theta):\n",
    "    R_fea_list = []\n",
    "    tot_input = (omega_list[:, np.newaxis, :, np.newaxis] * n_unweighted_inputs[np.newaxis, :, :, :]).sum(axis = 2)\n",
    "    null_spike_count = np.zeros(tot_input.shape[:-1])\n",
    "    for ith_bin in range(tot_input.shape[-1]):\n",
    "        null_spike_mask = tot_input[:,:,ith_bin] >= theta\n",
    "        null_spike_count += null_spike_mask\n",
    "        mem_len = min(ref_memory_len, tot_input.shape[-1] - ith_bin)\n",
    "        tot_input[:,:,ith_bin:ith_bin+mem_len] += null_spike_mask[:,:,np.newaxis] * ref_kernel[:mem_len]\n",
    "    R_null_list = np.mean(null_spike_count, axis = 1)/(n_unweighted_inputs.shape[2]/1000)\n",
    "    #np.savez(\"R_null_list\", R_null_list, null_spike_count)\n",
    "    for feature in fea_inputs:\n",
    "        fea_unweighted_inputs = add_fea(n_unweighted_inputs, feature, T_fea)\n",
    "        fea_tot_input = (omega_list[:, np.newaxis, :, np.newaxis] * fea_unweighted_inputs[np.newaxis, :, :, :]).sum(axis = 2) \n",
    "        spike_count = np.zeros(fea_tot_input.shape[:-1])\n",
    "        for ith_bin in range(fea_tot_input.shape[-1]):\n",
    "            spike_mask = fea_tot_input[:,:,ith_bin] >= theta\n",
    "            spike_count += spike_mask\n",
    "            mem_len = min(ref_memory_len, fea_tot_input.shape[-1] - ith_bin)\n",
    "            fea_tot_input[:,:,ith_bin:ith_bin+mem_len] += spike_mask[:,:,np.newaxis] * ref_kernel[:mem_len]\n",
    "        fea_spike_count = spike_count - null_spike_count\n",
    "        R_fea_list.append(np.mean(fea_spike_count, axis = 1))\n",
    "        #np.savez(\"R_fea_list\", R_fea_list)\n",
    "    return R_fea_list, R_null_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to compute the neural responses, which will be used to plot learning curves.\n",
    "\n",
    "Filenames for the computed synaptic efficacies (omega) of different trainings are as follows:\n",
    "\n",
    "a. 1 feature 1 spike: latest_omega1F1S_list.npy\n",
    "\n",
    "b. 1 feature 5 spikes: latest_omega1F5S_list.npy\n",
    "\n",
    "c. 5 features 1 spike: latest_omega5F1S_list.npy\n",
    "\n",
    "d. 5 features different spikes: latest_omega5FmS_list.npy\n",
    "\n",
    "e. 5 features 5 spikes: latest_omega5F5S_list.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "omega = np.load(\"latest_omega1F1S_list.npy\") # updated omega list after training\n",
    "l_curve_inputs = np.load(\"learning_curve_inputs.npy\") # unweighted inputs for a list of probe trials (n = 100)\n",
    "fea_inputs = np.load(\"feature_inputs.npy\") # unweighted inputs of features\n",
    "theta = 1 # threshold\n",
    "\n",
    "R_fea_list, R_null_list = neural_response(omega, l_curve_inputs, fea_inputs, T_fea, theta)\n",
    "np.savez(\"neural_responses\", R_fea_list, R_null_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
